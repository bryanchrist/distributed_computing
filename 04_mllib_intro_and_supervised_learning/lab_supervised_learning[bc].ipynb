{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Lab: Supervised Learning\n",
    "### Last Updated: August 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "This project has two parts:\n",
    "- Part I: Classification - build and apply a logistic regression model on the Wisconsin Breast Cancer dataset.\n",
    "- Part II: Regression - build and apply a linear regression model on the California Housing dataset.\n",
    "\n",
    "**Total Possible Points: 10**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Classification (5 POINTS)\n",
    "\n",
    "Here are the specifications and grading breakdown:\n",
    "\n",
    "- the target variable is `diagnosis`\n",
    "- use `f1`, `f2` as predictors (1 PT)\n",
    "- split data into 60% training set, 40% test set \n",
    "- standardize the predictors (1 PT)\n",
    "- use seed=314 whenever a seed is needed\n",
    "- fit a Logistic Regression model with an intercept (1 PT)\n",
    "- compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "DATA_FILEPATH = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.csv(DATA_FILEPATH,  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = training.randomSplit([0.6, 0.4], 314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+-----+-----+-----+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+--------+-------+--------+-----+-----+-----+------+-------+------+------+-------+------+-------+\n",
      "|  id|diagnosis|   f1|   f2|   f3|   f4|     f5|     f6|    f7|     f8|    f9|    f10|   f11|   f12|  f13|  f14|     f15|    f16|    f17|     f18|    f19|     f20|  f21|  f22|  f23|   f24|    f25|   f26|   f27|    f28|   f29|    f30|\n",
      "+----+---------+-----+-----+-----+-----+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+--------+-------+--------+-----+-----+-----+------+-------+------+------+-------+------+-------+\n",
      "|8670|        M|15.46|19.48|101.7|748.9| 0.1092| 0.1223|0.1466|0.08087|0.1931|0.05796|0.4743|0.7859|3.094|48.31| 0.00624|0.01484|0.02813| 0.01093|0.01397|0.002461|19.26| 26.0|124.9|1156.0| 0.1546|0.2394|0.3791| 0.1514|0.2837|0.08019|\n",
      "|8913|        B|12.89|13.12|81.89|515.9|0.06955|0.03729|0.0226|0.01171|0.1337|0.05581|0.1532| 0.469|1.115|12.68|0.004731|0.01345|0.01652|0.005905|0.01619|0.002081|13.62|15.54| 87.4| 577.0|0.09616|0.1147|0.1186|0.05366|0.2309|0.06915|\n",
      "+----+---------+-----+-----+-----+-----+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+--------+-------+--------+-----+-----+-----+------+-------+------+------+-------+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCols=['diagnosis'], outputCols=['diagnosis_numeric'], handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\"],\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='diagnosis_numeric',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages = [stringIndexer, assembler, scaler, lr])\n",
    "# Fit the model\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-----+-----+------+-------+-------+-------+-------+------+-------+------+-----+------+-----+--------+-------+-------+--------+-------+--------+-----+-----+-----+------+------+------+------+-------+------+-------+-----------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|   id|diagnosis|   f1|   f2|   f3|    f4|     f5|     f6|     f7|     f8|    f9|    f10|   f11|  f12|   f13|  f14|     f15|    f16|    f17|     f18|    f19|     f20|  f21|  f22|  f23|   f24|   f25|   f26|   f27|    f28|   f29|    f30|diagnosis_numeric|     features|      scaledFeatures|       rawPrediction|         probability|prediction|\n",
      "+-----+---------+-----+-----+-----+------+-------+-------+-------+-------+------+-------+------+-----+------+-----+--------+-------+-------+--------+-------+--------+-----+-----+-----+------+------+------+------+-------+------+-------+-----------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "| 9047|        B|12.94|16.17|83.18| 507.6|0.09879|0.08836|0.03296| 0.0239|0.1735|  0.062|0.1458|0.905|0.9975|11.36|0.002887|0.01285|0.01613|0.007308| 0.0187|0.001972|13.86|23.02|89.69| 580.9|0.1172|0.1958| 0.181|0.08388|0.3297|0.07834|              0.0|[12.94,16.17]|[3.62025881986513...|[0.59947660255067...|[0.64553655201103...|       0.0|\n",
      "|86208|        M|20.26|23.03|132.4|1264.0|0.09078| 0.1313| 0.1465|0.08683|0.2095|0.05649|0.7576|1.509| 4.554|87.87|0.006016|0.03482|0.04232| 0.01269|0.02657|0.004411|24.22|31.59|156.1|1750.0| 0.119|0.3539|0.4098| 0.1573|0.3689|0.08368|              1.0|[20.26,23.03]|[5.66819503017524...|[-0.2005856758348...|[0.45002104244720...|       1.0|\n",
      "+-----+---------+-----+-----+-----+------+-------+-------+-------+-------+------+-------+------+-----+------+-----+--------+-------+-------+--------+-------+--------+-----+-----+-----+------+------+------+------+-------+------+-------+-----------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(test)\n",
    "predDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC Curve: 0.6959459459459459\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"diagnosis_numeric\",\n",
    "                                          metricName=\"areaUnderROC\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "auc = evaluator.evaluate(predDF)\n",
    "\n",
    "print(\"Area under ROC Curve:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Regression (5 POINTS)\n",
    "\n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Here are the specifications and grading breakdown:\n",
    "\n",
    "- Scale the response variable median_house_value, dividing by 100000 (1 PT)\n",
    "\n",
    "- Split data into train set (80%), test set (20%) using seed=314 (1 PT)\n",
    "\n",
    "- Add new predictor: `rooms_per_household`\n",
    "\n",
    "- In the training set, select all of these features and standardize them: (1 PT)\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "- Fit a linear regression model on the training set with these parameters:\n",
    "\n",
    "  - maxIter=10\n",
    "  - regParam=0.3\n",
    "  - elasticNetParam=0.8  \n",
    "\n",
    "\n",
    "- Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH2 = 'cal_housing_data_preproc_w_header.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|          358500.0|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(DATA_FILEPATH2,  inferSchema=True, header = True)\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|             4.526|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|             3.585|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"median_house_value\", data['median_house_value']/100000)\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|             4.526|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|  6.984126984126984|\n",
      "|             3.585|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|  6.238137082601054|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('rooms_per_household', data['total_rooms']/data['households'])\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.8, 0.2], 314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_household\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feats,\n",
    "                            outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "# instantiate the model\n",
    "reg = LinearRegression(labelCol='median_house_value',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler, scaler, reg])\n",
    "# Fit the model\n",
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+-----------------+\n",
      "|median_house_value|     median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|            features|      scaledFeatures|       prediction|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+-----------------+\n",
      "|           0.14999|            4.1932|              52.0|      803.0|         267.0|     628.0|     225.0|   34.24|  -117.86|  3.568888888888889|[267.0,628.0,225....|[0.63739996917453...|2.159373789302095|\n",
      "|             0.225|0.7916999999999998|              52.0|      107.0|          79.0|     167.0|      53.0|   37.95|  -121.29|  2.018867924528302|[79.0,167.0,53.0,...|[0.18859399836999...|1.218189800777783|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(test)\n",
    "predDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 0.7551749809615463\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol = 'prediction',\n",
    "    labelCol = 'median_house_value',\n",
    "    metricName= 'mse')\n",
    "mse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"MSE is {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
