{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Tuning and Topic Modeling\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 20, 2023\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**  \n",
    "In this assignment, you will do three things:\n",
    "1) Tune a logistic regression model  \n",
    "2) Label-balance a dataset  \n",
    "3) Run the Topic Modeling notebook, making small tweaks and capturing results  \n",
    "\n",
    "**TOTAL POINTS: 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"data preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update to match your path\n",
    "directory_path = '/home/brc4cb/distributed_computing/07_tuning_and_nlp_task/'\n",
    "full_path_to_file = os.path.join(directory_path, 'breast_cancer_wisconsin.csv')\n",
    "path_to_data = os.path.join(full_path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class = 2 for benign (negative class, 4 for malignant (positive class)\n",
    "target = 'class'\n",
    "positive_label = 4\n",
    "negative_label = 2\n",
    "\n",
    "SEED = 314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca = spark.read.csv(path_to_data, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- clump_thickness: integer (nullable = true)\n",
      " |-- uniformity_cell_size: integer (nullable = true)\n",
      " |-- uniformity_cell_shape: integer (nullable = true)\n",
      " |-- marginal_adhesion: integer (nullable = true)\n",
      " |-- single_epithelial_cell_size: integer (nullable = true)\n",
      " |-- bare_nuclei: string (nullable = true)\n",
      " |-- bland_chromatin: integer (nullable = true)\n",
      " |-- normal_nucleoli: integer (nullable = true)\n",
      " |-- mitoses: integer (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brca.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brca.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  241|\n",
      "|    2|  458|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute distribution of target variable\n",
    "brca.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:  Cross Validate a Logistic Regression Model\n",
    "i) (**4 PTS**) This task has the following requirements:\n",
    "- import necessary modules\n",
    "- use these features as predictors: `clump_thickness`,`uniformity_cell_size`,`uniformity_cell_shape`,`marginal_adhesion`  \n",
    "- `class` is response variable. apply recoding as needed. hint: save as new variable.\n",
    "- use 3 folds in the cross validator object\n",
    "- use BinaryClassificationEvaluator\n",
    "- logistic regression model with `maxIter`=10  \n",
    "- tuning grid with `regParam` values of 0.1 and 0.01\n",
    "- finally, print the average metrics based on each `regParam` value. the attribute `avgMetrics` in the cv model will hold these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9903113414409844, 0.9901615314117947]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "brca = brca.withColumn(\"label\", when(brca['class'] == 4,1) \\\n",
    "      .when(brca['class'] == 2,0))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"clump_thickness\", \"uniformity_cell_size\", \"uniformity_cell_shape\", \"marginal_adhesion\"], outputCol=\"features\")\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, labelCol = 'label', featuresCol= 'scaledFeatures')\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler, scaler, lr])\n",
    "# Fit the model\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cvModel = crossval.setParallelism(4).fit(brca) # train 4 models in parallel\n",
    "print(cvModel.avgMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:  Balancing a DataFrame with Downsampling  \n",
    "i) (**2 PTS**) Write a function to implement downsampling.  Enter code into the cell containing the `downsample` function.  \n",
    "\n",
    "INPUTS  \n",
    "* df               - Spark dataframe  \n",
    "* target           - string, target variable  \n",
    "* positive_label   - integer, value of positive label  \n",
    "* negative_label   - integer, value of negative label  \n",
    "\n",
    "OUTPUT  \n",
    "balanced spark dataframe  \n",
    "\n",
    "Downsampling = sample from larger class to match smaller class  \n",
    "\n",
    "**Example:**  \n",
    "\n",
    "INITIAL STATE  \n",
    "Smaller class has 100 records  \n",
    "Larger class size has 400 records\n",
    "\n",
    "ACTION  \n",
    "Sample 100 records from larger class, without replacement  \n",
    "Retain all records from smaller class\n",
    "\n",
    "END STATE    \n",
    "This produces a balanced dataset containing 100 records from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "import random\n",
    "# Create an RDD with 500 rows and a unique ID for each row\n",
    "data = [(i,) for i in range(500)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = [\"id\"]\n",
    "\n",
    "# Create a DataFrame by mapping the RDD and adding the \"id\" column\n",
    "df_test = rdd.toDF(schema)\n",
    "\n",
    "# Add a \"label\" column with a value of 1 for rows with id < 400, and 0 for id >= 400\n",
    "df_test = df_test.withColumn(\"label\", lit(1)).filter(df_test[\"id\"] < 400).union(\n",
    "    df_test.withColumn(\"label\", lit(0)).filter(df_test[\"id\"] >= 400)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_sample_spark(df, target, positive_label, negative_label):\n",
    "    positive_count = df.filter(df[f'{target}']==positive_label).count()\n",
    "    negative_count = df.filter(df[f'{target}']==negative_label).count()\n",
    "    total = df.count()\n",
    "    positive_fraction = positive_count/total\n",
    "    negative_fraction = negative_count/total\n",
    "    pos_to_neg = positive_count/negative_count #when neg>pos, so upsample pos (or downsample neg)\n",
    "    neg_to_pos = negative_count/positive_count #when neg<pos, so downsample pos\n",
    "    if positive_fraction < negative_fraction: #upsample pos/downsample neg\n",
    "        df2 = df.filter(df[f\"{target}\"]==negative_label).sample(fraction = pos_to_neg, withReplacement=False)\n",
    "        df_final = df.filter(df[f'{target}']==positive_label).union(df2) \n",
    "    if positive_fraction > negative_fraction: #downsample\n",
    "        df2 = df.filter(df[f\"{target}\"]==positive_label).sample(fraction = neg_to_pos, withReplacement=False)\n",
    "        df_final = df.filter(df[f'{target}']==negative_label).union(df2) \n",
    "    return df_final\n",
    "        \n",
    "df = dynamic_sample_spark(df_test, 'label', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_sample_exact(df, target, positive_label, negative_label):\n",
    "    positive_count = df.filter(df[f'{target}']==positive_label).count()\n",
    "    negative_count = df.filter(df[f'{target}']==negative_label).count()\n",
    "    if positive_count < negative_count: #upsample\n",
    "        data = []\n",
    "        for i in range(0, negative_count-positive_count):\n",
    "            data.append(df.filter(df[f'{target}']==positive_label).collect()[random.randint(0, positive_count)])\n",
    "        df2 = spark.createDataFrame(data = data)\n",
    "    if positive_count > negative_count: #downsample\n",
    "        data = []\n",
    "        while len(data)<negative_count:\n",
    "            obs = df.filter(df[f'{target}']==positive_label).collect()[random.randint(0, positive_count)]\n",
    "            if obs not in data:\n",
    "                data.append(obs)\n",
    "        df2 = spark.createDataFrame(data = data)\n",
    "    df_final = df.filter(df[f'{target}']==negative_label).union(df2) \n",
    "    return df_final\n",
    "        \n",
    "df2 = dynamic_sample_exact(df_test, 'label', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) **(1 PT)** Print the target distribution from this balanced dataset, to show the label counts nearly match.\n",
    "\n",
    "#### IMPORTANT NOTE:\n",
    "Sampling won't produce the exact fraction you request. In order to sample efficiently, Spark uses Bernouilli Sampling. \n",
    "Each row is assigned a probability of being included. If you request a 10% sample, each row individually has a 10% chance of being included but this does not guarantee an exact 10% sample   \n",
    "(it should be close, however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  100|\n",
      "|    1|  108|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  100|\n",
      "|    1|  100|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote two functions, one using spark functionality and one using the random package in python. Both dynamically upsample or downsample based on the distribution of the labels in the dataframe, but the one that utilizes the random package in python returns a dataframe where the counts of both classes are perfectly balanced, whereas the spark implementation returns a dataframe where the counts for both classes are close, but not exact. I know this is extra work, but I had fun thinking through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Topic Modeling\n",
    "\n",
    "In this exercise, you will run the `topic_modeling.ipynb` notebook and answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) **(1 PT)** For the first headline in the dataset, the code processes it and extracts tokens. Provide a list of the tokens.\n",
    "\n",
    "+-------------+--------------------------------------------------+  \n",
    "|publish_date | headline_text                                     |  \n",
    "+-------------+--------------------------------------------------+  \n",
    "|20030219     | aba decides against community broadcasting licence|  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[aba, decid, commun, broadcast, licenc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) **(1 PT)** The code created a count vectorizer and extracted features. \n",
    "\n",
    "`cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)`\n",
    "\n",
    "The first document had six tokens and the feature vector looked like this:\n",
    "\n",
    "(500, [118, 498], [1.0, 1.0])   \n",
    "\n",
    "Explain why there are only two non-zero elements in this feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because you set the minimum document frequency to 3, meaning tokens that do not appear 3 or more times are not included in the 500-word vocabulary you set. As a result, the four other tokens in the document must not have appeared 3 or more times, so they were not added to the vocabulary and therefore they would not be in the feature vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) **(1 PT)** Change the number of topics to 2, rerun LDA, and visualize the topics by showing the topic words."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "topic: 0\n",
    "*************************\n",
    "iraq\n",
    "war\n",
    "govt\n",
    "sai\n",
    "claim\n",
    "win\n",
    "polic\n",
    "u\n",
    "council\n",
    "call\n",
    "*************************\n",
    "topic: 1\n",
    "*************************\n",
    "u\n",
    "new\n",
    "man\n",
    "polic\n",
    "charg\n",
    "plan\n",
    "iraqi\n",
    "fire\n",
    "kill\n",
    "nsw\n",
    "*************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
